[llama2]
hidden_size = 4096
num_heads = 32
intermediate_size = 11008
norm_eps = 1e-05
num_layers = 32
vocab_size = 32000
max_ctx_length = 4096
bos_token_id = 1
eos_token_id = 2

[model]
type = llama
model_file = llama2.q4.bin

[tokenizer]
type = bpe
model_file = llama2.tokenizer.bin
add_prefix_space = true
split_by_unicode = true

