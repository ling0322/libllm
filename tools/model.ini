[llama]
hidden_size = 4096
num_heads = 32
intermediate_size = 11008
norm_eps = 1e-05
num_layers = 32
vocab_size = 32000
max_ctx_length = 4096
bos_token_id = 1
eot_token_id = 2

[model]
type = llama
model_file = model.bin

