[chatglm2]
hidden_size = 4096
vocab_size = 65024
kv_channels = 128
seq_length = 32768
hidden_size_per_attention_head = 128
multi_query_group_num = 2
norm_eps = 1e-05
ffn_hidden_size = 13696
num_layers = 28
symbol_gmask = 64790
symbol_sop = 64792
symbol_eos = 2

[model]
type = chatglm2
model_file = chatglm2.q4.bin

[tokenizer]
type = bpe
model_file = chatglm2.tokenizer.bin
add_prefix_space = true
split_by_unicode = true

