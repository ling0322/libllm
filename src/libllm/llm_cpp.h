// The MIT License (MIT)
//
// Copyright (c) 2023 Xiaoyang Chen
//
// Permission is hereby granted, free of charge, to any person obtaining a copy of this software
// and associated documentation files (the "Software"), to deal in the Software without
// restriction, including without limitation the rights to use, copy, modify, merge, publish,
// distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the
// Software is furnished to do so, subject to the following conditions:
// 
// The above copyright notice and this permission notice shall be included in all copies or
// substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
// BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
// NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

// C++ wrapper for libllm C API.

#pragma once

#include <memory>
#include <sstream>
#include <stdexcept>
#include <string>
#include "llm.h"

namespace llm {

class Model;
class ModelFactory;
class Completion;

enum class DeviceType {
  CPU = LLM_DEVICE_CPU,
  CUDA = LLM_DEVICE_CUDA,
  AUTO = LLM_DEVICE_AUTO
};

// configuration for LLM completion task.
class CompletionConfig {
 public:
  CompletionConfig() :
      _topP(0.8f),
      _topK(50),
      _temperature(1.0f) {}

  // setters for the config.
  void setTopP(float topP) { _topP = topP; }
  void setTopK(int topK) { _topK = topK; }
  void setTemperature(float temperature) { _temperature = temperature; }

  // getters for the config.
  float getTopP() const { return _topP; }
  int getTopK() const { return _topK; }
  float getTemperature() const { return _temperature; }

 private:
  float _topP;
  int _topK;
  float _temperature;
};

class Chunk {
 public:
  friend class Completion;

  std::string getText() const { return _text; }

 private:
  std::string _text;
};

/// @brief Store the state of ongoing completion task.
class Completion {
 public:
  friend class Model;

  /// @brief If completion is ongoing (active) returns true, if stopped returns false.
  /// @return If completion is active.
  bool isActive();

  /// @brief Get the next chunk of tokens generated by the model.
  /// @return instance of Chunk.
  Chunk nextChunk();

 private:
  std::shared_ptr<llmCompletion_t> _completion;
  std::shared_ptr<llmChunk_t> _chunk;
  const llmApi_t *_api;

  Completion() : _api(nullptr) {}
};

/// @brief Input prompt for Model::comeplete().
class Prompt {
 public:
  friend class Model;

  /// @brief Append text to the prompt.
  /// @param text text to append.
  void appendText(const std::string &text);

  /// @brief Append a control token to the prompt.
  /// @param text name of the control token.
  void appendControlToken(const std::string &text);

 private:
  const llmApi_t *_api;
  std::shared_ptr<llmPrompt_t> _prompt;
  
  Prompt() : _api(nullptr) {}
};

/// @brief Stores an instance of LLM Model.
class Model {
 public:
  friend class ModelFactory;

  /// @brief Get the name of model, for example, "llama".
  /// @return name of the model.
  std::string getName();

  /// @brief Create a prompt.
  /// @return pointer of prompt.
  std::shared_ptr<Prompt> createPrompt() const;

  /// @brief Complete the string version of given `prompt` with LLM.
  /// @param prompt The prompt to complete.
  /// @param config The config for completion.
  /// @return A `Completion` object.
  std::shared_ptr<Completion> complete(std::shared_ptr<Prompt> prompt,
                                       CompletionConfig config = CompletionConfig());

 private:
  std::shared_ptr<llmModel_t> _model;
  const llmApi_t *_api;

  Model() : _api(nullptr) {}
};

class ModelFactory {
 public:
  // Use this constructor when libLLM is dynamic loaded (dlopen).
  ModelFactory(const llmApi_t *api) : _api(api) {
    if (!api) throw std::runtime_error("api is empty");
  }

  /// @brief Create an instance of Model from the config file path;
  /// @param configFile config file of the model.
  /// @param device device of the model storage and computation device. Use DeviceType::AUTO to
  /// let libllm determine the best one.
  /// @return A shared pointer of the Model instance.
  std::shared_ptr<Model> createModel(const std::string &config,
                                     DeviceType device = DeviceType::AUTO);


 private:
  const llmApi_t *_api;
};

// -- Implementation of libLLM C++ API (wrapper for C api) ----------------------------------------

namespace internal {

inline void throwLastError(const llmApi_t *api) {
  std::string lastError = api->getLastErrorMessage();
  throw std::runtime_error(lastError);
}

}  // namespace internal

// -- Completion ----------

inline bool Completion::isActive() {
  return _api->isActive(_completion.get()) != 0;
}

inline Chunk Completion::nextChunk() {
  if (LLM_OK != _api->getNextChunk(_completion.get(), _chunk.get())) {
    internal::throwLastError(_api);
  }

  const char *text = _api->getChunkText(_chunk.get());
  if (!text) internal::throwLastError(_api);

  Chunk c;
  c._text = text;
  return c;
}

// -- Prompt ----------

inline void Prompt::appendText(const std::string &text) {
  if (LLM_OK != _api->appendText(_prompt.get(), text.c_str())) {
    internal::throwLastError(_api);
  }
}

inline void Prompt::appendControlToken(const std::string &name) {
  if (LLM_OK != _api->appendControlToken(_prompt.get(), name.c_str())) {
    internal::throwLastError(_api);
  }
}

// -- Model ----------

inline std::string Model::getName() {
  const char *name = _api->getModelName(_model.get());
  if (!name) internal::throwLastError(_api);

  return name;
}

inline std::shared_ptr<Prompt> Model::createPrompt() const {
  std::shared_ptr<llmPrompt_t> pPrompt(_api->createPrompt(_model.get()),
                                       _api->destroyPrompt);
  if (!pPrompt) internal::throwLastError(_api);

  std::shared_ptr<Prompt> prompt{new Prompt()};
  prompt->_api = _api;
  prompt->_prompt = pPrompt;

  return prompt;
}

inline std::shared_ptr<Completion> Model::complete(std::shared_ptr<Prompt> prompt,
                                                   CompletionConfig config) {
  std::shared_ptr<llmCompletion_t> pComp(_api->createCompletion(_model.get()),
                                         _api->destroyCompletion);
  if (!pComp) internal::throwLastError(_api);

  if (LLM_OK != _api->setPrompt(pComp.get(), prompt->_prompt.get())) internal::throwLastError(_api);
  if (LLM_OK != _api->setTopK(pComp.get(), config.getTopK())) internal::throwLastError(_api);
  if (LLM_OK != _api->setTopP(pComp.get(), config.getTopP())) internal::throwLastError(_api);
  if (LLM_OK != _api->setTemperature(pComp.get(), config.getTemperature())) {
    internal::throwLastError(_api);
  }
  if (LLM_OK != _api->startCompletion(pComp.get())) internal::throwLastError(_api);

  std::shared_ptr<llmChunk_t> pChunk(_api->createChunk(), _api->destroyChunk);
  std::shared_ptr<Completion> comp{new Completion()};
  comp->_api = _api;
  comp->_chunk = pChunk;
  comp->_completion = pComp;

  return comp;
}

inline std::shared_ptr<Model> ModelFactory::createModel(const std::string &config,
                                                        DeviceType device) {
  std::shared_ptr<llmModel_t> pModel(_api->createModel(), _api->destroyModel);
  int32_t dwDevice = static_cast<int32_t>(device);
  if (LLM_OK != _api->setModelFile(pModel.get(), config.c_str())) internal::throwLastError(_api);
  if (LLM_OK != _api->setModelDevice(pModel.get(), dwDevice)) internal::throwLastError(_api);
  if (LLM_OK != _api->loadModel(pModel.get())) internal::throwLastError(_api);

  std::shared_ptr<Model> model{new Model()};
  model->_api = _api;
  model->_model = pModel;

  return model;
}

}  // namespace llm