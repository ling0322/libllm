# libLLM: Efficient inference of large language models.

[![Linux](https://github.com/ling0322/libllm/actions/workflows/cmake-linux.yml/badge.svg?branch=main)](https://github.com/ling0322/libllm/actions/workflows/cmake-linux.yml) [![Windows](https://github.com/ling0322/libllm/actions/workflows/cmake-windows.yml/badge.svg?branch=main)](https://github.com/ling0322/libllm/actions/workflows/cmake-windows.yml) [![macOS](https://github.com/ling0322/libllm/actions/workflows/cmake-darwin.yml/badge.svg?branch=main)](https://github.com/ling0322/libllm/actions/workflows/cmake-darwin.yml)

Welcome to libLLM, an open-source project designed for efficient inference of large language models (LLM) on ordinary personal computers and mobile devices. The core is implemented in C++14, without any third-party dependencies (such as BLAS or SentencePiece), enabling seamless operation across a variety of devices.

æ¬¢è¿ä½¿ç”¨libLLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåœ¨æ™®é€šä¸ªäººç”µè„‘å’Œç§»åŠ¨è®¾å¤‡ä¸Šé«˜æ•ˆæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è€Œè®¾è®¡çš„å¼€æºé¡¹ç›®ã€‚æ ¸å¿ƒä½¿ç”¨C++14ç¼–å†™ï¼Œæ²¡æœ‰ç¬¬ä¸‰æ–¹ä¾èµ–ï¼ˆBLASã€SentencePieceç­‰ï¼‰ï¼Œèƒ½åœ¨å„ç§è®¾å¤‡ä¸­æ— ç¼è¿è¡Œã€‚

## Model download:

| Model       | Download       |
|-------------|----------------|
| Index-1.9B-Character (Role-playing) | ğŸ¤—[Huggingface](https://huggingface.co/ling0322/bilibili-index-1.9b-libllm/blob/main/bilibili-index-1.9b-character-q4.llmpkg) |
| Index-1.9B-Chat | ğŸ¤—[Huggingface](https://huggingface.co/ling0322/bilibili-index-1.9b-libllm/blob/main/bilibili-index-1.9b-chat-q4.llmpkg) |

## Recent updates

- [2024-08-02] Support the translation command in llm.
- [2024-07-30] Support model downloading from huggingface. For example, `llm chat -model index-character` will automatically download the `index-character` model from ğŸ¤—[Huggingface](https://huggingface.co/ling0322/bilibili-index-1.9b-libllm/blob/main/bilibili-index-1.9b-chat-q4.llmpkg).

## Quickstart

To run and chat with Bilibili-Index-1.9B-Character:

```bash
$ llm chat -m index-character
```

## Key features:

- Optimized for everyday devices: libLLM has been optimized to run smoothly on common personal computers, ensuring the powerful capabilities of large language models are accessible to a wider range of users.
- C++ code: Written in standard C++14, it is simple and efficient.
- No external dependencies: The core functionality does not require third-party dependencies (BLAS, SentencePiece, etc.), and the necessary GEMM kernels are implemented internally (avx2, avx512).
- CUDA support: Supports accelerated inference using CUDA.

## ç‰¹ç‚¹

- ä¸ºæ—¥å¸¸è®¾å¤‡è¿›è¡Œä¼˜åŒ–ï¼šlibLLMç»è¿‡ä¼˜åŒ–ï¼Œå¯åœ¨å¸¸è§çš„ä¸ªäººç”µè„‘ä¸Šå¹³ç¨³è¿è¡Œï¼Œç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½é¢å‘æ›´å¹¿æ³›çš„ç”¨æˆ·ã€‚
- C++ä»£ç ï¼šé‡‡ç”¨æ ‡å‡†C++14ç¼–å†™ï¼Œç®€å•é«˜æ•ˆã€‚
- æ— å¤–éƒ¨ä¾èµ–ï¼šæ ¸å¿ƒåŠŸèƒ½æ— éœ€ç¬¬ä¸‰æ–¹ä¾èµ–ï¼ˆBLASã€SentencePieceç­‰ï¼‰ï¼Œæ‰€éœ€çš„GEMMå†…æ ¸å‡åœ¨å†…éƒ¨å®ç°(avx2ã€avx512)ã€‚
- æ”¯æŒCUDAï¼šæ”¯æŒä½¿ç”¨CUDAåŠ é€Ÿæ¨ç†ã€‚

## Build

### libLLM CPU only

```bash
$ mkdir build && cd build
$ cmake ..
$ make -j
```

#### For macOS

Please brew install OpenMP before cmake. NOTE: currently libllm macOS expected to be very slow since there is no aarch64 kernel for it.

```bash
% brew install libomp
% export OpenMP_ROOT=$(brew --prefix)/opt/libomp
% mkdir build && cd build
% cmake ..
% make -j
```

### Build with CUDA

NOTE: specify `-DCUDAToolkit_ROOT=<CUDA-DIR>` if there is multiple CUDA versions in your OS.

Recommand versions are:
- CUDA: 11.7

```bash
$ mkdir build && cd build
$ cmake -DWITH_CUDA=ON [-DCUDAToolkit_ROOT=<CUDA-DIR>] ..
$ make -j
```

## Run libllm command line

```bash
$ src/libllm/llm chat -m index-character
INFO 2024-07-30T12:02:28Z interface.cc:67] ISA support: AVX2=1 F16C=1 AVX512F=1
INFO 2024-07-30T12:02:28Z interface.cc:71] Use Avx512 backend.
INFO 2024-07-30T12:02:30Z matmul.cc:43] Use GEMM from cuBLAS.
INFO 2024-07-30T12:02:30Z cuda_operators.cc:51] cuda numDevices = 2
INFO 2024-07-30T12:02:30Z cuda_operators.cc:52] cuda:0 maxThreadsPerMultiProcessor = 2048
INFO 2024-07-30T12:02:30Z cuda_operators.cc:54] cuda:0 multiProcessorCount = 20
INFO 2024-07-30T12:02:30Z thread_pool.cc:73] ThreadPool started. numThreads=20
INFO 2024-07-30T12:02:30Z llm.cc:204] read model package: /home/xiaoych/.libllm/models/bilibili-index-1.9b-character-q4.llmpkg
INFO 2024-07-30T12:02:30Z model_for_generation.cc:43] model_type = index
INFO 2024-07-30T12:02:30Z model_for_generation.cc:44] device = cuda
INFO 2024-07-30T12:02:31Z state_map.cc:66] 220 tensors read.
Please input your question.
    Type ':new' to start a new session (clean history).
    Type ':sys <system_prompt>' to set the system prompt and start a new session .
> hi
æ‚¨å¥½ï¼æˆ‘æ˜¯Indexï¼Œè¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ
(12 tokens, time=0.76s, 63.47ms per token)
> 
```

## API Examples

### Python

```python
from libllm import Model, ControlToken

model = Model("tools/bilibili_index.llmpkg")
prompt = [ControlToken("<|reserved_0|>"), "hi", ControlToken("<|reserved_1|>")]

for chunk in model.complete(prompt):
    print(chunk.text, end="", flush=True)

print("\nDone!")
```

### Go

```go
package main

import (
    "fmt"
    "log"

    "github.com/ling0322/libllm/go/llm"
)

func main() {
    model, err := llm.NewModel("../../tools/bilibili_index.llmpkg", llm.Auto)
    if err != nil {
        log.Fatal(err)
    }

    prompt := llm.NewPrompt()
    prompt.AppendControlToken("<|reserved_0|>")
    prompt.AppendText("hi")
    prompt.AppendControlToken("<|reserved_1|>")
    comp, err := model.Complete(llm.NewCompletionConfig(), prompt)
    if err != nil {
        log.Fatal(err)
    }

    for comp.IsActive() {
        chunk, err := comp.GenerateNextChunk()
        if err != nil {
            log.Fatal(err)
        }

        fmt.Print(chunk.Text)
    }
    fmt.Println()
}

```

## Export Huggingface models

Here is an example of exporting Index-1.9B model from huggingface.

```bash
$ cd tools
$ python bilibili_index_exporter.py \
    -huggingface_name IndexTeam/Index-1.9B-Character \
    -quant q4  \
    -output index.llmpkg 

```

Then all required modules realted to `IndexTeam/Index-1.9B-Character`, including model, tokenizer and configs will be written to `index.llmpkg`.
