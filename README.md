# libLLM: Efficient inference of large language models.

[![Linux](https://github.com/ling0322/libllm/actions/workflows/cmake-linux.yml/badge.svg?branch=main)](https://github.com/ling0322/libllm/actions/workflows/cmake-linux.yml) [![Windows](https://github.com/ling0322/libllm/actions/workflows/cmake-windows.yml/badge.svg?branch=main)](https://github.com/ling0322/libllm/actions/workflows/cmake-windows.yml)

Welcome to libLLM, an open-source project designed for efficient inference of large language models (LLM) on ordinary personal computers and mobile devices. The core is implemented in C++14, without any third-party dependencies (such as BLAS or SentencePiece), enabling seamless operation across a variety of devices.

æ¬¢è¿Žä½¿ç”¨libLLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåœ¨æ™®é€šä¸ªäººç”µè„‘å’Œç§»åŠ¨è®¾å¤‡ä¸Šé«˜æ•ˆæŽ¨ç†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰è€Œè®¾è®¡çš„å¼€æºé¡¹ç›®ã€‚æ ¸å¿ƒä½¿ç”¨C++14ç¼–å†™ï¼Œæ²¡æœ‰ç¬¬ä¸‰æ–¹ä¾èµ–ï¼ˆBLASã€SentencePieceç­‰ï¼‰ï¼Œèƒ½åœ¨å„ç§è®¾å¤‡ä¸­æ— ç¼è¿è¡Œã€‚

## Key features:

- Optimized for Everyday Devices: libLLM is finely tuned for smooth operation on common personal computers, ensuring the powerful capabilities of large language models are accessible to a broader user base.
- C++ Codebase: The core is written in standard C++14, facilitating straightforward compilation.
- No External Dependencies: With no reliance on third-party dependencies such as BLAS or SentencePiece, libLLM internally implements the necessary GEMM kernels (avx2, avx512). 

## ç‰¹ç‚¹

- ä¸ºæ—¥å¸¸è®¾å¤‡è¿›è¡Œä¼˜åŒ–ï¼šlibLLMç»è¿‡ä¼˜åŒ–ï¼Œå¯åœ¨å¸¸è§çš„ä¸ªäººç”µè„‘ä¸Šå¹³ç¨³è¿è¡Œï¼Œç¡®ä¿å¤§åž‹è¯­è¨€æ¨¡åž‹çš„å¼ºå¤§åŠŸèƒ½é¢å‘æ›´å¹¿æ³›çš„ç”¨æˆ·ã€‚
- C++ä»£ç ï¼šæ ¸å¿ƒé‡‡ç”¨æ ‡å‡†C++14ç¼–å†™ï¼Œå¯ç›´æŽ¥ç¼–è¯‘ã€‚
- æ— å¤–éƒ¨ä¾èµ–ï¼šæ— éœ€ç¬¬ä¸‰æ–¹ä¾èµ–ï¼ˆBLASã€SentencePieceç­‰ï¼‰ï¼Œæ‰€éœ€çš„GEMMå†…æ ¸å‡åœ¨å†…éƒ¨å®žçŽ°(avx2ã€avx512)ã€‚

## Supported models:

| Model            | Download |
|------------------|----------|
| Llama2           | [HuggingFace](https://huggingface.co/ling0322/llama2-7b-libllm-q4/tree/main) |
| ChatGLM2-6b      | [HuggingFace](https://huggingface.co/ling0322/chatglm2-6b-libllm-q4/tree/main) |

## Build

```bash
$ mkdir build && cd build
$ cmake ..
$ make -j
```

## Run libllm command line

```bash
$ ./src/llm/llm -config ../model/chatglm3-6b-libllm-q4/chatglm3.config 
INFO 2023-12-19T08:56:47Z lymath.cc:42] lymath: Use Avx512 backend.
INFO 2023-12-19T08:56:48Z cuda_operators.cc:46] cuda numDevices = 1
INFO 2023-12-19T08:56:48Z cuda_operators.cc:47] cuda:0 maxThreadsPerMultiProcessor = 2048
INFO 2023-12-19T08:56:48Z cuda_operators.cc:49] cuda:0 multiProcessorCount = 20
INFO 2023-12-19T08:56:48Z llm.cc:123] OMP max_threads = 20
INFO 2023-12-19T08:56:48Z bpe_model.cc:34] read tokenizer from ../model/chatglm3-6b-libllm-q4/chatglm3.tokenizer.bin
INFO 2023-12-19T08:56:48Z model_factory.cc:35] model_type = chatglm3
INFO 2023-12-19T08:56:48Z model_factory.cc:36] device = cuda
INFO 2023-12-19T08:56:48Z state_map.cc:58] read state map from ../model/chatglm3-6b-libllm-q4/chatglm3.q4.bin
INFO 2023-12-19T08:56:51Z state_map.cc:68] reading ... 100.0%
INFO 2023-12-19T08:56:51Z state_map.cc:69] 200 tensors read.
> ä½ å¥½
 
 ä½ å¥½ðŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿Žé—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
(29 token, time=0.92s, 31.75ms per token)
> 
```

## API Examples

### Python

```python
from libllm import Model, SpecialToken

model = Model("model/chatglm3-6b-libllm-q4/chatglm3.config")
prompt = [SpecialToken("<|user|>"), "\n", "ä½ å¥½", SpecialToken("<|assistant|>")]

for chunk in model.complete(prompt):
    print(chunk.text, end="", flush=True)

print("\nDone!")
```

## Export Huggingface models

Here is an example of exporting ChatGLM2 model from huggingface.

```bash
$ cd tools
$ python chatglm_exporter.py
```

Then 3 files will be exported: `chatglm3.config`, `chatglm3.q4.bin` and `chatglm3.tokenizer.bin`
