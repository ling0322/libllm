# libLLM: Efficient inference of large language models.

[![Linux](https://github.com/ling0322/libllm/actions/workflows/cmake-linux.yml/badge.svg?branch=main)](https://github.com/ling0322/libllm/actions/workflows/cmake-linux.yml)

Welcome to libLLM, an open-source project tailored for efficient inference of large language models (LLMs) on everyday PCs and mobile devices. This library is dedicated to providing powerful inference capabilities to various LLMs, enabling them to seamlessly run on a variety of devices.

æ¬¢è¿Žä½¿ç”¨libLLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåœ¨æ™®é€šä¸ªäººç”µè„‘å’Œç§»åŠ¨è®¾å¤‡ä¸Šé«˜æ•ˆæŽ¨ç†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰è€Œè®¾è®¡çš„å¼€æºé¡¹ç›®ã€‚è¯¥åº“ä¸“æ³¨äºŽä¸ºå„ç§LLMæä¾›å¼ºå¤§çš„æŽ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å„ç§è®¾å¤‡ä¸­æ— ç¼è¿è¡Œã€‚

## Supported models:

- ChatGLM2
- Llama2

## Key Features:

- Optimization for Everyday Devices: libLLM is optimized to run smoothly on common PCs and mobile devices, ensuring that the power of large language models is accessible to a wide range of users.
- Versatile Inference Capabilities: The library is dedicated to providing powerful inference capabilities for diverse LLMs, catering to a variety of applications and use cases.
- Native C++ Codebase: The core functionality is implemented in C++ code, allowing for direct compilation without the need for third-party dependencies.
- No External Dependencies: Enjoy hassle-free integration as libLLM operates without relying on third-party dependencies, simplifying the deployment process.

## ç‰¹ç‚¹:

- ä¸ºæ—¥å¸¸è®¾å¤‡è¿›è¡Œä¼˜åŒ–ï¼š libLLMç»è¿‡ä¼˜åŒ–ï¼Œå¯åœ¨å¸¸è§çš„ä¸ªäººç”µè„‘å’Œç§»åŠ¨è®¾å¤‡ä¸Šå¹³ç¨³è¿è¡Œï¼Œç¡®ä¿å¤§åž‹è¯­è¨€æ¨¡åž‹çš„å¼ºå¤§åŠŸèƒ½é¢å‘æ›´å¹¿æ³›çš„ç”¨æˆ·ã€‚
- å¤šåŠŸèƒ½çš„æŽ¨ç†èƒ½åŠ›ï¼š è¯¥åº“è‡´åŠ›äºŽä¸ºå„ç§LLMæä¾›å¼ºå¤§çš„æŽ¨ç†èƒ½åŠ›ï¼Œæ»¡è¶³å„ç§åº”ç”¨å’Œç”¨ä¾‹çš„éœ€æ±‚ã€‚
- æœ¬æœºC++ä»£ç ï¼š æ ¸å¿ƒåŠŸèƒ½é‡‡ç”¨C++ç¼–å†™ï¼Œå¯ç›´æŽ¥ç¼–è¯‘ï¼Œæ— éœ€ç¬¬ä¸‰æ–¹ä¾èµ–ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½ã€‚
- æ— å¤–éƒ¨ä¾èµ–ï¼š libLLMåœ¨è¿è¡Œæ—¶æ— éœ€ä¾èµ–ç¬¬ä¸‰æ–¹åº“ï¼ˆæ¯”å¦‚BLASï¼‰ï¼Œç®€åŒ–äº†éƒ¨ç½²è¿‡ç¨‹ï¼Œä½¿é›†æˆæ›´åŠ è½»æ¾ã€‚

## Build

```bash
$ mkdir build && cd build
$ cmake ..
$ make -j
```

## Run libllm command line

```c++
$ build/src/libllm/llm/llm --ini tools/chatglm2.config 
INFO 2023-10-20T08:58:55Z lymath.cc:44] lymath: Use Avx512 backend.
INFO 2023-10-20T08:58:55Z state_map.cc:58] read state map from tools/chatglm2.q4.bin
INFO 2023-10-20T08:58:58Z state_map.cc:68] reading ... 100.0%
INFO 2023-10-20T08:58:58Z state_map.cc:69] 200 tensors read.
> ä½ å¥½
 ä½ å¥½ðŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿Žé—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
> 
```

## API Examples

### Python

```python
import libllm

model = libllm.Model("model/chatglm2-6b-q4/chatglm2.config")
prompt = "[Round 1]\n\né—®ï¼šä½ å¥½\n\nç­”ï¼š"

for chunk in libllm.Completion(model, prompt):
    print(chunk.text, end="", flush=True)

print("\nDone!")
```

## Export Huggingface models

Here is an example of exporting ChatGLM2 model from huggingface.

```bash
$ cd tools
$ python chatglm2_exporter.py
```

Then 3 files will be exported: `chatglm2.config`, `chatglm2.q4.bin` and `chatglm2.tokenizer.bin`
